japanese_paragraph
"
タイトルからある通り、黒本と呼ばれている参考書があり、こいつは最強です。
これ1冊あれば十分って言われてましたが、受け終わった後に確かにそうだったなと感じております。
理由としては、

章末にある演習問題の類題が結構な割合で出題される
解説が豊富で、先に問題を解くスタイルなので飽きにくい
各章の練習問題が意外と演習問題や本番に活きる部分がある
この辺りかなと思っています。
"
"勉強方法
あくまでも私自身のやり方ではありましたが、こんな感じでした。

黒本1週目→とりあえず解説を見ずに問題を解き、そのあと解説を見ながら答え合わせ
黒本2週目→解説や演習に掲載されているコードを打ち込んで動かす
黒本3週目→演習問題を解いてから、正解以外の選択肢の理由を考えたり調べたりする
黒本4-7週目→ひたすら問題を解く
あとはウォームアップ的な感じで、ほぼ毎日演習問題を解く前に練習を全て解いてからやってました。

お気づきかもしれませんが、解説を読んでから解くのではなく、

ともかく手を動かす。何が何でも手を動かす

これが大事です。

参考書は読んでると飽きます。
コード書いてた方が楽しいに決まってるじゃないですか！"
"お疲れ様です！
現在スケジュールがタイトな案件に関わっているため
残業対策として、基本の勤務時間を7時～16時にさせてください。
その上で毎日30分～1時間ほど残業して18時頃に退勤する予定です。"
"おはようございます。
急に申し訳ありませんが、体調不良のため本日お休みとさせていただきます。
よろしくお願いいたします。"
"Redisとは
Redisは、先述の通り、キー・バリューストア（KVS）型のNoSQLです。

IT用語辞典 e-Wordsによると

KVS（Key-Value Store）とは、データ管理システムの種類の一つで、保存したいデータ（value：値）に対し、対応する一意の標識（key：キー）を設定し、これらをペアで格納する方式。

だそうです。

また、同じキー・バリューストア型としては、AmazonのDynamoDBがありますね。"
"大規模言語モデル（LLM）とは
大規模言語モデル（LLM：Large language Models）とは、大量のデータとディープラーニング（深層学習）技術によって構築された言語モデルです。言語モデルは文章や単語の出現確率を用いてモデル化したものであり、文章作成などの自然言語処理で用いられています。大規模言語モデルと従来の言語モデルでは、「データ量」「計算量」「パラメータ量」が大きく異なります。

データ量：入力される情報量
計算量：コンピューターが処理する計算量
パラメータ量：確率計算を行うための係数量
大規模言語モデルでは上記の3点が大幅に増加したことで、精度が格段に向上しました。大規模言語モデルは、2017年に発表された「Transformer」がきっかけとなって構築されたといわれています。Transformerの登場によるブレイクスルーと、マシンパワーの向上によって、より多くのデータをモデルに学習させられるようになったことで大規模言語モデルは誕生しました。"
"大規模言語モデル（LLM）の仕組み
大規模言語モデルは、おおまかに表すと次のような仕組みで構築されています。

トークン化：入力文を最小単位に分別
文脈理解：プロンプト内の各トークンとの関連性を計算
エンコード：特徴量の抽出
デコード：次のトークンを予測
入力文の次のトークンの確率を出力
大規模言語モデルは基本的にTransformerの仕組みを利用しています。従来の言語モデルでは、テキストデータであれば単語に分割した後に人がラベル付けをする必要がありました。しかし、大規模言語モデルでは大量のテキストデータを与えることで、トークンから文脈や言葉の意味を学習できます。この学習した結果から、特定の言葉に続く確率が高いと考えられる言葉・文章を並べられるものが大規模言語モデルなのです。"
"大規模言語モデル（LLM）の仕組み
大規模言語モデルは、おおまかに表すと次のような仕組みで構築されています。

トークン化：入力文を最小単位に分別
文脈理解：プロンプト内の各トークンとの関連性を計算
エンコード：特徴量の抽出
デコード：次のトークンを予測
入力文の次のトークンの確率を出力
大規模言語モデルは基本的にTransformerの仕組みを利用しています。従来の言語モデルでは、テキストデータであれば単語に分割した後に人がラベル付けをする必要がありました。しかし、大規模言語モデルでは大量のテキストデータを与えることで、トークンから文脈や言葉の意味を学習できます。この学習した結果から、特定の言葉に続く確率が高いと考えられる言葉・文章を並べられるものが大規模言語モデルなのです。

大規模言語モデル（LLM）の種類
大規模言語モデルの礎となったTransformerをもとに開発された有名な大規模言語モデルを紹介します。

BERT
BERT（Bidirectional Encoder Representations from Transformesrs）は、2018年にGoogleの論文で発表された自然言語処理モデルです。日本語では「Transformerによる双方向のエンコード表現」と訳されます。BERTは文章を文頭と文末（双方向）から学習することで「文脈を読める」ようになりました。翻訳や質問応答などの自然言語処理タスクにおいて、2018年当時では最高スコアを記録しています。

GPT-3
「ChatGPT」として多くの方が認知しているサービスでは、GPT（Generative Pre-trained Transformer）と呼ばれる大規模言語モデルが利用されています。GPTは大量のテキストデータを事前学習した後に、特定のタスクに適用させるファインチューニングと呼ばれる学習をする2段階の学習モデルです。
GPT-3はOpenAI社によって2020年に発表され、ChatGPTではチャット向けにファインチューニングしたGPT-3.5が利用されています。

GPT-4
GPT-4は2023年にアップデートされたGPTの最新版であり、テキストだけでなく画像などの入力を受け取ってテキストを出力できる「マルチモーダル」なモデルです。GPT-3.5で扱えるトークンの最大数は4,097であったのに対し、GPT-4では3万2,768トークンと約8倍に増えています。そのため、GPT-3.5よりも複雑な質問にも回答できるようになりました。"
"AGI（Artificial General Intelligence：汎用的人工知能）はまだ存在しない
AIに対する最も大きな誤解が、「あらゆる問題に応えられる（答えられる）賢いAIがすでに存在する」というものです。現在のAIに人間の認識能力や常識、感情なども含めた森羅万象のすべてを理解させられるわけではなく、「何でもできる」存在ではありません。
あらゆる課題に対応できる汎用的なAIを実現することは研究者にとって大きな目標であり、さまざまなアイデアが提唱されていますが、ブレークスルーの決め手となるような手法は見つかっていません。その意味でもAIはまだ進化の途上にあります。"
"いま最も注目されているテクノロジーの1つに人工知能（AI：Artificial Intelligence）があります。AIは、一般的には「人が実現するさまざまな知覚や知性を人工的に再現するもの」という意味合いで理解されています。
しかし実際には、AIに対して一意に決まった定義がなされているわけではありません。コンピューター・サイエンスや認知科学、医学、心理学、さらには哲学にいたるまで、今もさまざまな立場で論じられ続けている領域です。"
"Prompt Engineering Guide
プロンプトエンジニアリングは、言語モデル（LMs）を効率的に使用するためのプロンプトを開発および最適化する比較的新しい学問分野です。プロンプトエンジニアリングのスキルを身につけることで、大規模言語モデル（LLMs）の能力と限界をより理解することができます。

研究者は、プロンプトエンジニアリングを使用して、質問応答や算術推論などの一般的なおよび複雑なタスクのLLMsの能力を向上させます。開発者は、LLMsやその他のツールとのインタフェースとなる強固で効果的なプロンプテクニックを設計するためにプロンプトエンジニアリングを使用します。

プロンプトエンジニアリングは、プロンプトの設計と開発に限らず、LLMsとのインタラクションおよび開発に役立つ幅広いスキルと技術を含みます。これは、LLMsとインタフェースすること、ビルドすること、能力を理解することに重要なスキルであり、LLMsの安全性を向上させたり、ドメイン知識や外部ツールを使用してLLMsの機能を拡張するためにプロンプトエンジニアリングを使用できます。

LLMsでの開発に高い関心があることから、この新しいプロンプトエンジニアリングガイドを作成しました。最新の論文、学習ガイド、モデル、講義、参考文献、新しいLLMの機能、およびプロンプトエンジニアリングに関連するツールがすべて含まれています。"
"Few-Shotプロンプティング
大規模言語モデルは、驚くべきゼロショット能力を示していますが、ゼロショット設定を使用した場合には、より複雑なタスクで不十分になることがあります。Few-shot promptingは、プロンプト内のデモを提供してモデルをより高い性能に導く文脈学習を可能にするテクニックとして使用できます。このデモンストレーションは、その後のモデルに反応を起こさせる例のための条件付けとなります。"
"Wei et al. (2022)で紹介されたchain-of-thought (CoT)プロンプティングは、中間的な推論ステップを介して複雑な推論能力を可能にします。few-shot promptingと組み合わせることで、推論が必要なより複雑なタスクでより良い結果を得ることができます。

プロンプト:

このグループの奇数を合計すると偶数になります。: 4、8、9、15、12、2、1。
A: 奇数を全て加えると(9, 15, 1)25になります。答えはFalseです。
このグループの奇数を合計すると偶数になります。: 17、10、19、4、8、12、24。
A: 奇数を全て加えると(17, 19)36になります。答えはTrueです。
このグループの奇数を合計すると偶数になります。: 16、11、14、4、8、13、24。
A: 奇数を全て加えると(11, 13)24になります。答えはTrueです。
このグループの奇数を合計すると偶数になります。: 17、9、10、12、13、4、2。
A: 奇数を全て加えると(17, 9, 13)39になります。答えはFalseです。
このグループの奇数を合計すると偶数になります。: 15、32、5、13、82、7、1。
A:

出力:

奇数を全て加えると(15, 5, 13, 7, 1)41になります。答えはFalseです。"
"1. RAG（Retrieval-Augmented Generation：検索拡張生成）とは？
RAGとは、LLM（大規模言語モデル）※のテキスト生成に、信頼性の高い外部情報の検索を組み合わせることで、プロンプトだけではコントロールしづらい出力精度を向上させるフレームワークです。

検索（Retrieval）機能を拡張（Augmented）し、質の高い回答を生成（Generation）できるようになることから、それぞれの頭文字を取って「RAG」と呼ばれています。

RAGではLLMが回答を生成する前段階に、最新の情報や専門分野のデータベースなどの外部情報（外付けの情報）を付与し、それらを検索できる工程を追加することで、LLMのウィークポイントを克服しつつ、エビデンスが明確で精度の高い出力が可能になります。

※LLM（Large Language Models｜大規模言語モデル）：膨大なデータとディープラーニング技術によってトレーニングされた自然言語処理モデルのこと。人間が話す言葉や書く文章などを学習して単語の出現率を統計的に分析し、学習したデータをもとにテキスト生成や文章要約などを行う技術。"
"LangchainによるLLMの進化：意思決定を可能にする""Agent""モデル
①Agentとは
""Agent""はモデルに意思決定の機能を与えるものです。具体的に言うと、モデルが状況によって、適切なAPIやツールを活用して問題を解決する能力を提供します。
Agentを活用すればLLMが持っている多くの弱みを克服できます。
例えば、LLMは学習データに含まれていない情報に対しては適切に回答することができず、誤った情報を提供する（幻覚問題とも呼ばれる）傾向がありますが、Agentなら解決できます。
APIを使って、自分でインターネットに接続し、必要な情報を検索すれば、学習されたことない情報に関する問題にも対処できるようになりますね。
さらに、どのような状況でローカルのデータセットを利用して検索するか、どのような状況でインターネットを使うべきかまでも全部「Agent」が考えて作業します。"
"AIエージェントが必要ないくつかの主要な理由
目標志向の行動: LLMsとRAGモデルは、主に彼らのトレーニングデータのパターンに基づいて人間らしいテキストを生成することに焦点を当ててるが、柔軟で知的な方法で具体的な目標を設定し追求する能力が欠けています。一方、AIエージェントは明確な目標を持ち、それらの目標を達成するために計画を立て行動を取る能力を持つように設計することができます。

メモリと状態の追跡: ほとんどの現行の言語モデルには持続的なメモリや状態追跡の能力がありません。各入力は独立して処理されます。一方、AIエージェントは内部状態を維持し、時間の経過とともに知識を蓄積し、その状態を活用して将来の意思決定や行動に影響を与えることができます。

環境との相互作用: LLMはテキスト領域でのみ動作し、物理世界との直接的な相互作用はありません。一方、AIエージェントは環境を認識し、その環境に対応する行動を取ることができます。それがデジタル世界、ロボットシステム、またはセンサーやアクチュエータを介しての物理世界であってもです。

転送と一般化: LLMsは、彼らのトレーニングデータに類似した言語タスクに優れていますが、完全に新しいドメインやタスクに知識を転送することが難しいことがよくあります。一方、学習、推論、計画の能力を持つAIエージェントは、新しい状況に対する転送と一般化の可能性があります。

継続的な学習: ほとんどの言語モデルはトレーニング後に静的に運用されます。一方、AIエージェントは新しい環境や状況との相互作用を通じて知識とスキルを継続的に学習し適応させることができます。

マルチタスク能力: LLMは通常、特定の言語タスクに特化しています。一方、AIエージェントは言語、推論、認識、制御などのさまざまなスキルを柔軟に組み合わせて複雑で多面的な問題に取り組むことができる一般的なマルチタスクシステムとして設計することができます。"
"1. SVMの概要
　サポートベクターマシーン(SVM)は2値分類問題を解くために考えられた学習アルゴリズムであり、基本的には線形の識別器です。しかし、カーネル関数と最適化手法の組み合わせにより非線形な分類問題も解けるように拡張することができます。 SVMはハードマージンSVMとソフトマージンSVMの2種類に分けられます。ハードマージンは最も基本的なSVMで、データを完全に分類することを想定しています。実際には完全にデータを分類することができないことがあるため、分類できない部分を許す大きさの変数を導入したSVMをソフトマージンSVMと言います。どちらのSVMも線型分類を想定して作られたSVMで、これらとは別に非線形に拡張が可能です。

　SVMの特徴として局所解にはまることがないという利点があります。学習に利用する目的関数が凸関数であるので局所解の問題がなくなります。しかし、非線形問題を解く場合のカーネル関数によっては局所解を持つ可能性があります。"
"k-means法概要
k-means法とは何か
k-means法は、まずデータを適当なクラスタに分けた後、クラスタの平均を用いてうまい具合にデータがわかれるように調整させていくアルゴリズムです。任意の指定のk個のクラスタを作成するアルゴリズムであることから、k-means法(k点平均法と呼ばれています。)

k-means法のアルゴリズム
k-mean法は具体的には下記のような工程を辿ります。

各点
に対してランダムにクラスタを割り振る
各クラスタに割り当てられた点について重心を計算する
各点について上記で計算された重心からの距離を計算し、距離が一番近いクラスタに割り当て直す。
2.と3.の工程を、割り当てられるクラスタが変化しなくなるまで行う
図で表現すると下記のように(a)→(b)→(c)→(d)のような順序を辿ってクラスタが収束していくイメージです。
(b)の段階でまず各点に適当にクラスタが割り振られ、その重心が計算されます(重心は赤星で図示)。(c)ではその重心との距離のもとに再度クラスタが割り当てられます。(新しい重心を赤星で図示、古い重心を薄い赤星で図示)。この工程を繰り返し(d)のようにクラスタが変化しないようなかたちに収束すれば完了です。"
"画像生成AIとは？
画像生成AIとは、ユーザーが入力したテキストを頼りに、AIがオリジナルの画像を数秒～数十秒程度で自動生成するシステムを指します。日本でよく知られている画像生成AIには「Stable Diffusion（ステーブルディフュージョン）」や「Midjourney（ミッドジャーニー）」があり、デザイン業界の常識を覆す存在として注目を浴びています。

これまで自分自身で画像を作成できなかったユーザーや、画像素材サイト等で月額利用料を払って高品質な画像をダウンロードしていたユーザーにとって、画像生成AIはメリットの大きいものとして捉えられる傾向にあります。しかし、画像や絵を作り出してきたクリエイターや、風景や人物を撮影してきたカメラマンにとっては脅威と認識されており、今後のデジタルクリエイティブに大きな影響を与える存在となっています。"
"Stable Diffusion 3 Medium
Stable Diffusion 3 Medium は Stable Diffusion 3 series の最新かつ最も高度なテキストから画像へのAIモデルで、20億のパラメータで構成されています。フォトリアリズムに優れ、複雑なプロンプトを処理し、明瞭なテキストを生成します。ウェイトはオープンな非商用ライセンスでご利用いただけます。商用利用の場合は、ライセンスの詳細についてお問い合わせください。

コードをダウンロード
Stable Assistant で試す"